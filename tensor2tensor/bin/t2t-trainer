#!/usr/bin/env python
# coding=utf-8
# Copyright 2017 The Tensor2Tensor Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

r"""Trainer for T2T models.

This binary perform training, evaluation, and inference using
the Estimator API with tf.learn Experiment objects.

To train your model, for example:
  t2t-trainer \
      --data_dir ~/data \
      --problems=algorithmic_identity_binary40 \
      --model=transformer
      --hparams_set=transformer_base
"""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import fathomt2t
import fathomairflow.dags.dag_management.xcom_manipulation as xcom
from gcloud.download_data import upload_to_gcs
from gcloud.gcs_registry import get_gcs_location

import os
import time
import shutil

# Dependency imports

from tensor2tensor.utils import registry
from tensor2tensor.utils import trainer_utils
from tensor2tensor.utils import usr_dir
from tensorflow.python.training import saver

import tensorflow as tf

flags = tf.flags
FLAGS = flags.FLAGS

# See trainer_utils.py for additional command-line flags.
flags.DEFINE_string("t2t_usr_dir", "",
                    "Path to a Python module that will be imported. The "
                    "__init__.py file should include the necessary imports. "
                    "The imported files should contain registrations, "
                    "e.g. @registry.register_model calls, that will then be "
                    "available to the t2t-trainer.")
flags.DEFINE_string("tmp_dir", "/tmp/t2t_datagen",
                    "Temporary storage directory.")
flags.DEFINE_bool("generate_data", False, "Generate data before training?")

flags.DEFINE_integer("eval_steps", 10, "Number of steps in evaluation.")
flags.DEFINE_string("output_dir", "", "Base output directory for run.")
flags.DEFINE_string("master", "", "Address of TensorFlow master.")
flags.DEFINE_string("schedule", "train_and_evaluate",
                    "Method of tf.contrib.learn.Experiment to run.")


##################
#
# FATHOM ADDITIONS
#
##################
flags.DEFINE_bool("debug_mode", False, "Truncate training for debug purposes")
flags.DEFINE_bool("profile", False, "Profile performance?")

def main(_):
  tf.logging.set_verbosity(tf.logging.INFO)
  usr_dir.import_usr_dir(FLAGS.t2t_usr_dir)
  trainer_utils.log_registry()
  trainer_utils.validate_flags()
  output_dir = os.path.expanduser(FLAGS.output_dir)
  tmp_dir = os.path.expanduser(FLAGS.tmp_dir)
  if not FLAGS.data_dir:
    raise ValueError("You must specify a --data_dir")
  data_dir = os.path.expanduser(FLAGS.data_dir)
  tf.gfile.MakeDirs(output_dir)

  # Generate data if requested.
  if FLAGS.generate_data:
    tf.gfile.MakeDirs(data_dir)
    tf.gfile.MakeDirs(tmp_dir)
    for problem_name in FLAGS.problems.split("-"):
      tf.logging.info("Generating data for %s" % problem_name)
      problem = registry.problem(problem_name)
      problem.generate_data(data_dir, tmp_dir)

  if FLAGS.debug_mode:
    FLAGS.train_steps = 1
    FLAGS.eval_steps = 1

  # Run the trainer.

  def run_experiment():
    trainer_utils.run(
      data_dir=data_dir,
      model=FLAGS.model,
      output_dir=output_dir,
      train_steps=FLAGS.train_steps,
      eval_steps=FLAGS.eval_steps,
      schedule=FLAGS.schedule)
  
  if FLAGS.profile:
    with tf.contrib.tfprof.ProfileContext('/usr/data/output/tfprof',
                                          trace_steps=range(100),
                                          dump_steps=range(100)) as pctx:
      opts = tf.profiler.ProfileOptionBuilder.time_and_memory()
      pctx.add_auto_profiling('op', opts, range(100))

      run_experiment()

  else:
    run_experiment()

  if not FLAGS.debug_mode and FLAGS.eval_early_stopping_steps is not None: 
    _pick_optimal_model()

  dir_path, model_name = _upload_model_to_gcs()

  xcom.echo_yaml_for_xcom_ingest({'output_dir': dir_path,
                                  'model_name': model_name})


def _upload_model_to_gcs() -> str:
  """Upload a model to Google cloud storage.

  Returns:
     dir_path: local directory where saved model is stored
     model_name: name to save model under in GCS
  """
  # assemble all checkpoint files for the checkpoint we want to use
  checkpoint_state = saver.get_checkpoint_state(FLAGS.output_dir)    
  checkpoint_path = checkpoint_state.model_checkpoint_path
  files = os.listdir(FLAGS.output_dir)
  checkpoint_file = os.path.basename(checkpoint_path)
  files = [x for x in files if x.startswith(checkpoint_file + '.')] + ['checkpoint']

  # create a dir to export
  dir_path = os.path.join(FLAGS.output_dir, 'export')
  
  # Remove dir if it exists. We create it first so that rmtree doesn't
  # die if it doesn't exist. We could instead do a try-catch but this
  # is nice because a) it's simple, and b) it still fails when rmtree
  # is unable to delete the directory for some reason.
  os.makedirs(dir_path, exist_ok=True)
  shutil.rmtree(dir_path)
  os.makedirs(dir_path, exist_ok=False)

  # move all checkpoint files for the chosen checkpoint into dir
  for path in files:
    os.rename(os.path.join(FLAGS.output_dir, path),
              os.path.join(dir_path, path))

  # Create a model name that includes both the model name and the
  # timestamp
  model_name = _model_name(FLAGS.model, FLAGS.output_dir)
    
  _upload_model_dir_to_gcs(dir_path, model_name)

  return dir_path, model_name

def _model_name(model_type:str, output_dir:str) -> str:
  """Creates a model name based on current time and model type

  Args:
    model_type: type of the model create (ie. blstm, cnn)
  """
  # remove the trailing slash if it is there
  norm_output_dir = os.path.normpath(output_dir)

  # get the last directory in the path
  dirname = os.path.basename(norm_output_dir)
  
  return "{}_{}".format(model_type, dirname)
    

def _upload_model_dir_to_gcs(dir_path:str, model_name:str) -> None:
  """
  Tars and uploads a directory with all the files required for a model

  Args:
    dir_path: path where all the model files are 
    model_name: a unique name for a model
  """
  tar_name = "{}.tar.gz".format(model_name)
  location = get_gcs_location(namespace='T2T_MODELS',
          path=tar_name)
  upload_to_gcs(location, dir_path)


def _pick_optimal_model() -> None:
  """Update the checkpoint so that it points to the best model that was
  encountered during training. Here "best" is defined as the lowest or
  highest value of the chosen early stopping metric. (By default,
  lowest loss.)

  We do this automatically based on knowledge of how early stopping
  works; i.e., we take the model that prevented early stopping from
  stopping before it did.
  """
  checkpoint_state = saver.get_checkpoint_state(FLAGS.output_dir)
  all_checkpoint_paths = list(checkpoint_state.all_model_checkpoint_paths)

  def extract_step(path):
    """Extract the step number from a checkpoint path

    Args:
        path: a path, e.g., model.ckpt-17

    Returns:
        step: the step number as an int, e.g., 17
    """
    return int(path[path.rindex('-') + 1:])

  # get available step numbers
  steps = [(extract_step(path), path) for path in all_checkpoint_paths]
  steps = sorted(steps)
  steps, all_checkpoint_paths = zip(*steps)
  all_checkpoint_paths = list(all_checkpoint_paths)
  
  # the step we want is the last one that would have allowed us to
  # stop when we did (at steps[-1])
  thresh = steps[-1] - FLAGS.eval_early_stopping_steps

  # get the last step that is <= thresh. Note that the early
  # stopping flags are phrased in terms of step number, not how many
  # times we've run eval.
  best_step_index = [ step <= thresh for step in steps ].index(False) - 1
  assert best_step_index >= 0, 'Early stopping stopped before it should have'

  # this is the checkpoint we want
  checkpoint_path = all_checkpoint_paths[best_step_index]

  # hack FIXME TODO XXX
  # We are about to move the best model into an export folder
  dirname, basename = os.path.split(checkpoint_path)
  checkpoint_path = os.path.join(dirname, 'export', basename)
  
  print('Early stopping chose checkpoint', checkpoint_path)
  
  saver.update_checkpoint_state(
    FLAGS.output_dir,
    checkpoint_path,
    all_checkpoint_paths + [checkpoint_path])
  
    
if __name__ == "__main__":
  tf.app.run(main)
