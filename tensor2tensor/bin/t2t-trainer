#!/usr/bin/env python
# coding=utf-8
# Copyright 2017 The Tensor2Tensor Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

r"""Trainer for T2T models.

This binary perform training, evaluation, and inference using
the Estimator API with tf.learn Experiment objects.

To train your model, for example:
  t2t-trainer \
      --data_dir ~/data \
      --problems=algorithmic_identity_binary40 \
      --model=transformer
      --hparams_set=transformer_base
"""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import fathomt2t
from gcloud.download_data import upload_file_to_gcs
from gcloud.gcs_registry import get_gcs_location

import os
import time

# Dependency imports

from tensor2tensor.utils import registry
from tensor2tensor.utils import trainer_utils
from tensor2tensor.utils import usr_dir
from tensorflow.python.training import saver

import tensorflow as tf

flags = tf.flags
FLAGS = flags.FLAGS

# See trainer_utils.py for additional command-line flags.
flags.DEFINE_string("t2t_usr_dir", "",
                    "Path to a Python module that will be imported. The "
                    "__init__.py file should include the necessary imports. "
                    "The imported files should contain registrations, "
                    "e.g. @registry.register_model calls, that will then be "
                    "available to the t2t-trainer.")
flags.DEFINE_string("tmp_dir", "/tmp/t2t_datagen",
                    "Temporary storage directory.")
flags.DEFINE_bool("generate_data", False, "Generate data before training?")

flags.DEFINE_integer("eval_steps", 10, "Number of steps in evaluation.")
flags.DEFINE_string("output_dir", "", "Base output directory for run.")
flags.DEFINE_string("master", "", "Address of TensorFlow master.")
flags.DEFINE_string("schedule", "train_and_evaluate",
                    "Method of tf.contrib.learn.Experiment to run.")


##################
#
# FATHOM ADDITIONS
#
##################
flags.DEFINE_bool("debug_mode", False, "Truncate training for debug purposes")
flags.DEFINE_bool("profile", False, "Profile performance?")

def main(_):
  tf.logging.set_verbosity(tf.logging.INFO)
  usr_dir.import_usr_dir(FLAGS.t2t_usr_dir)
  trainer_utils.log_registry()
  trainer_utils.validate_flags()
  output_dir = os.path.expanduser(FLAGS.output_dir)
  tmp_dir = os.path.expanduser(FLAGS.tmp_dir)
  if not FLAGS.data_dir:
    raise ValueError("You must specify a --data_dir")
  data_dir = os.path.expanduser(FLAGS.data_dir)
  tf.gfile.MakeDirs(output_dir)

  # Generate data if requested.
  if FLAGS.generate_data:
    tf.gfile.MakeDirs(data_dir)
    tf.gfile.MakeDirs(tmp_dir)
    for problem_name in FLAGS.problems.split("-"):
      tf.logging.info("Generating data for %s" % problem_name)
      problem = registry.problem(problem_name)
      problem.generate_data(data_dir, tmp_dir)

  if FLAGS.debug_mode:
    FLAGS.train_steps = 1
    FLAGS.eval_steps = 1

  # Run the trainer.

  def run_experiment():
    trainer_utils.run(
      data_dir=data_dir,
      model=FLAGS.model,
      output_dir=output_dir,
      train_steps=FLAGS.train_steps,
      eval_steps=FLAGS.eval_steps,
      schedule=FLAGS.schedule)
  
  if FLAGS.profile:
    with tf.contrib.tfprof.ProfileContext('/usr/data/output/tfprof',
                                          trace_steps=range(100),
                                          dump_steps=range(100)) as pctx:
      opts = tf.profiler.ProfileOptionBuilder.time_and_memory()
      pctx.add_auto_profiling('op', opts, range(100))

      run_experiment()

  else:
    run_experiment()

  if not FLAGS.debug_mode and FLAGS.eval_early_stopping_steps is not None: 
    # update the checkpoint so that it points to the best model
    checkpoint_state = saver.get_checkpoint_state(FLAGS.output_dir)
    all_checkpoint_paths = list(checkpoint_state.all_model_checkpoint_paths)

    def extract_step(path):
      """Extract the step number from a checkpoint path

      Args:
          path: a path, e.g., model.ckpt-17

      Returns:
          step: the step number as an int, e.g., 17
      """
      return int(path[path.rindex('-') + 1:])

    # get available step numbers
    steps = [(extract_step(path), path) for path in all_checkpoint_paths]
    steps = sorted(steps)
    steps, all_checkpoint_paths = zip(*steps)
    all_checkpoint_paths = list(all_checkpoint_paths)
    
    # the step we want is the last one that would have allowed us to
    # stop when we did (at steps[-1])
    thresh = steps[-1] - FLAGS.eval_early_stopping_steps

    # get the last step that is <= thresh. Note that the early
    # stopping flags are phrased in terms of step number, not how many
    # times we've run eval.
    best_step_index = [ step <= thresh for step in steps ].index(False) - 1
    assert best_step_index >= 0, 'Early stopping stopped before it should have'

    # this is the checkpoint we want
    checkpoint_path = all_checkpoint_paths[best_step_index]

    print('Early stopping chose checkpoint', checkpoint_path)
    
    saver.update_checkpoint_state(
      FLAGS.output_dir,
      checkpoint_path,
      all_checkpoint_paths + [checkpoint_path])

  # assemble files we want
  checkpoint_state = saver.get_checkpoint_state(FLAGS.output_dir)    
  checkpoint_path = checkpoint_state.model_checkpoint_path
  files = os.listdir(FLAGS.output_dir)
  checkpoint_file = os.path.basename(checkpoint_path)
  files = [x for x in files if x.startswith(checkpoint_file + '.')] + ['checkpoint']

  # clean up dir if it exists, create if it does not
  model_name = _model_name(FLAGS.model)
  dir_path = os.path.join(FLAGS.output_dir, model_name)
  if os.path.exists(dir_path):
    stale_dir_files = [os.path.join(dir_path, x) for x in os.listdir(dir_path)]
    for path in stale_dir_files:
      os.remove(path)
  else:
    dir = os.makedirs(dir_path, exist_ok=False)

  # move files into dir
  for path in files:
    os.rename(os.path.join(FLAGS.output_dir, path),
              os.path.join(dir_path, path))

  _upload_to_gcs(dir_path, model_name)


def _model_name(model_type:str) -> str:
  """
  Creates a model name based on current time and model type

  Args:
    model_type: type of the model create (ie. blstm, cnn)
  """
  return "{}_{}".format(
      time.strftime("%Y%m%d-%H%M%S"),
      model_type)


def _upload_to_gcs(dir_path:str, model_name:str) -> None:
  """
  Tars and uploads a directory with all the files required for a model

  Args:
    dir_path: path where all the model files are 
    model_name: a unique name for a model
  """
  tar_name = "{}.tar.gz".format(model_name)
  location = get_gcs_location(namespace='T2T_MODELS',
          file_path=tar_name)
  upload_file_to_gcs(location, dir_path)
  
    
if __name__ == "__main__":
  tf.app.run(main)
